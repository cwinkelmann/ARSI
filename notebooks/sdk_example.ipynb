{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifed example of the hyp-sdk Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Using the HyP3 SDK for Python\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ASFHyP3/hyp3-sdk/main?filepath=docs%2Fsdk_example.ipynb)\n",
    "\n",
    "HyP3's Python SDK `hyp3_sdk` provides a convenience wrapper around the HyP3 API and HyP3 jobs.\n",
    "\n",
    "\n",
    "The HyP3 SDK can be installed using [Anaconda/Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html#anaconda-or-miniconda)\n",
    " (recommended) via [`conda`](https://anaconda.org/conda-forge/hyp3_sdk):\n",
    "\n",
    "```\n",
    "conda install -c conda-forge hyp3_sdk\n",
    "```\n",
    "\n",
    "Or using [`pip`](https://pypi.org/project/hyp3-sdk/):\n",
    "\n",
    "```\n",
    "python -m pip install hyp3_sdk\n",
    "```\n",
    "\n",
    "Full documentation of the SDK can be found in the [HyP3 documentation](https://hyp3-docs.asf.alaska.edu/using/sdk/).\n",
    "\n",
    "### Other recommended packages\n",
    "\n",
    "We also recommend installing the `asf_search` Python package for performing searches of the ASF catalog. The ASF Search\n",
    "Python package can be installed using [Anaconda/Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html#anaconda-or-miniconda)\n",
    "(recommended) via [`conda`](https://anaconda.org/conda-forge/asf_search):\n",
    "\n",
    "```\n",
    "conda install -c conda-forge asf_search\n",
    "```\n",
    "\n",
    "Or using [`pip`](https://pypi.org/project/asf-search):\n",
    "\n",
    "```\n",
    "python -m pip install asf_search\n",
    "```\n",
    "\n",
    "Full documentation of `asf_search` can be found in the [ASF search documentation](https://docs.asf.alaska.edu/asf_search/basics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyp3_sdk\n",
      "  Downloading hyp3_sdk-6.0.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from hyp3_sdk) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from hyp3_sdk) (2.31.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from hyp3_sdk) (2.0.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from hyp3_sdk) (4.66.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->hyp3_sdk) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->hyp3_sdk) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->hyp3_sdk) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->hyp3_sdk) (2024.2.2)\n",
      "Downloading hyp3_sdk-6.0.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: hyp3_sdk\n",
      "Successfully installed hyp3_sdk-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install  hyp3_sdk\n",
    "\n",
    "# !conda install -c conda-forge hyp3_sdk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "import asf_search as asf\n",
    "import hyp3_sdk as sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticating to the API\n",
    "\n",
    "The SDK will attempt to pull your [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) credentials out of `~/.netrc`\n",
    "by default, or you can pass your credentials in directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Was not able to authenticate with .netrc file and no credentials provided\nThis could be due to invalid credentials in .netrc or a connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/hyp3_sdk/util.py:101\u001b[0m, in \u001b[0;36mget_authenticated_session\u001b[0;34m(username, password)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://urs.earthdata.nasa.gov/oauth/authorize?response_type=code&client_id=BO_n7nTIlMljdvU6kRRB3g&redirect_uri=https://auth.asf.alaska.edu/login&app_type=401",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# .netrc\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hyp3 \u001b[38;5;241m=\u001b[39m \u001b[43msdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHyP3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/hyp3_sdk/hyp3.py:47\u001b[0m, in \u001b[0;36mHyP3.__init__\u001b[0;34m(self, api_url, username, password, prompt)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m password \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m prompt:\n\u001b[1;32m     45\u001b[0m     password \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNASA Earthdata Login password: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m \u001b[43mhyp3_sdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_authenticated_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyp3_sdk\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyp3_sdk\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/hyp3_sdk/util.py:103\u001b[0m, in \u001b[0;36mget_authenticated_session\u001b[0;34m(username, password)\u001b[0m\n\u001b[1;32m    101\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(auth_error_message)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Was not able to authenticate with .netrc file and no credentials provided\nThis could be due to invalid credentials in .netrc or a connection error."
     ]
    }
   ],
   "source": [
    "# .netrc\n",
    "hyp3 = sdk.HyP3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NASA Earthdata Login username:  karisuvtol\n",
      "NASA Earthdata Login password:  ········\n"
     ]
    }
   ],
   "source": [
    "# or enter your credentials\n",
    "hyp3 = sdk.HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting jobs\n",
    "\n",
    "The SDK provides a submit method for [all supported job types](https://hyp3-docs.asf.alaska.edu/products/).\n",
    "\n",
    "### Submitting Sentinel-1 RTC jobs\n",
    "\n",
    "Sentinel-1 Radiometric Terrain Correction (RTC) jobs are submitted using [ESA granule IDs](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/naming-conventions).\n",
    "The example granules below can be viewed  in [ASF Search here](https://search.asf.alaska.edu/#/?zoom=7.08772014623877&center=-141.733866,58.498008&resultsLoaded=true&granule=S1A_IW_SLC__1SDV_20210214T154835_20210214T154901_036588_044C54_8494-SLC&searchType=List%20Search&searchList=S1A_IW_SLC__1SDV_20210214T154835_20210214T154901_036588_044C54_8494-SLC,S1B_IW_SLC__1SDV_20210210T153131_20210210T153159_025546_030B48_B568-SLC,S1A_IW_SLC__1SDV_20210210T025526_20210210T025553_036522_0449E2_7769-SLC,S1A_IW_SLC__1SDV_20210210T025501_20210210T025528_036522_0449E2_3917-SLC,S1B_IW_SLC__1SDV_20210209T030255_20210209T030323_025524_030A8D_7E88-SLC,S1B_IW_SLC__1SDV_20210209T030227_20210209T030257_025524_030A8D_5BAF-SLC,S1A_IW_SLC__1SDV_20210202T154835_20210202T154902_036413_044634_01A1-SLC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 HyP3 Jobs: 0 succeeded, 0 failed, 0 running, 7 pending.\n"
     ]
    }
   ],
   "source": [
    "granules = [\n",
    "    'S1A_IW_SLC__1SDV_20210214T154835_20210214T154901_036588_044C54_8494',\n",
    "    'S1B_IW_SLC__1SDV_20210210T153131_20210210T153159_025546_030B48_B568',\n",
    "    'S1A_IW_SLC__1SDV_20210210T025526_20210210T025553_036522_0449E2_7769',\n",
    "    'S1A_IW_SLC__1SDV_20210210T025501_20210210T025528_036522_0449E2_3917',\n",
    "    'S1B_IW_SLC__1SDV_20210209T030255_20210209T030323_025524_030A8D_7E88',\n",
    "    'S1B_IW_SLC__1SDV_20210209T030227_20210209T030257_025524_030A8D_5BAF',\n",
    "    'S1A_IW_SLC__1SDV_20210202T154835_20210202T154902_036413_044634_01A1',\n",
    "]\n",
    "\n",
    "\n",
    "rtc_jobs = sdk.Batch()\n",
    "for g in granules:\n",
    "    rtc_jobs += hyp3.submit_rtc_job(g, name='rtc-example')\n",
    "print(rtc_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've given each job the name `rtc-example`, which we can use later to search for these jobs.\n",
    "\n",
    "`HyP3.submit_rtc_job` also accepts\n",
    "[keyword arguments](https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job)\n",
    "to customize the RTC products to your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Submitting Sentinel-1 InSAR jobs\n",
    "\n",
    "The SDK can also submit Sentinel-1 Interferometric Synthetic Aperture Radar (InSAR) jobs which processes\n",
    "reference and secondary granule pairs.\n",
    "\n",
    "For a particular reference granule, we may want to use the nearest and next-nearest temporal neighbor granules as secondary\n",
    "scenes. To programmatically find our secondary granules for a reference granule, We'll define a `get_nearest_neighbors`\n",
    "function that uses the [baseline stack](https://docs.asf.alaska.edu/asf_search/ASFProduct/#stack) method from `asf_search`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def get_nearest_neighbors(granule: str, max_neighbors: Optional[int] = None) -> asf.ASFSearchResults:\n",
    "    granule = asf.granule_search(granule)[-1]\n",
    "    stack = reversed([item for item in granule.stack() if item.properties['temporalBaseline'] < 0])\n",
    "    return asf.ASFSearchResults(stack)[:max_neighbors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, using the example granule list for our RTC jobs as the reference scenes, we can find their nearest and next-nearest neighbor granules, and submit them\n",
    "as pairs for InSAR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # For a nice progress bar: https://github.com/tqdm/tqdm#ipython-jupyter-integration\n",
    "\n",
    "insar_jobs = sdk.Batch()\n",
    "for reference in tqdm(granules):\n",
    "    neighbors = get_nearest_neighbors(reference, max_neighbors=2)\n",
    "    for secondary in neighbors:\n",
    "        insar_jobs += hyp3.submit_insar_job(reference, secondary.properties['sceneName'], name='insar-example')\n",
    "print(insar_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Like RTC jobs, `HyP3.submit_insar_job` accepts\n",
    "[keyword arguments](https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_insar_job)\n",
    "to customize the InSAR products to your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting autoRIFT jobs\n",
    "\n",
    "AutoRIFT supports processing Sentinel-1, Sentinel-2, or Landsat-8 Collection 2 pairs.\n",
    "* Sentinel-1 jobs are submitted using [ESA granule IDs](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/naming-conventions)\n",
    "* Sentinel-2 jobs are submitted using [ESA granule IDs](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention)\n",
    "* Landsat-8 Collection 2 jobs are submitted using [USGS scene IDs](https://www.usgs.gov/faqs/what-naming-convention-landsat-collection-2-level-1-and-level-2-scenes?qt-news_science_products=0#qt-news_science_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 HyP3 Jobs: 0 succeeded, 0 failed, 0 running, 1 pending.\n"
     ]
    }
   ],
   "source": [
    "autorift_pairs = [\n",
    "    # Sentinel-1 ESA granule IDs\n",
    "    ('S1A_IW_SLC__1SSH_20170221T204710_20170221T204737_015387_0193F6_AB07',\n",
    "     'S1B_IW_SLC__1SSH_20170227T204628_20170227T204655_004491_007D11_6654'),\n",
    "    # Sentinel-2 ESA granule IDs\n",
    "    ('S2B_MSIL1C_20200612T150759_N0209_R025_T22WEB_20200612T184700',\n",
    "     'S2A_MSIL1C_20200627T150921_N0209_R025_T22WEB_20200627T170912'),\n",
    "    # Landsat 8\n",
    "    ('LC08_L1TP_009011_20200703_20200913_02_T1',\n",
    "     'LC08_L1TP_009011_20200820_20200905_02_T1'),\n",
    "]\n",
    "\n",
    "\n",
    "autorift_pairs = [\n",
    "    # Sentinel-1 ESA granule IDs\n",
    "    ('S1A_IW_SLC__1SSH_20210308T082926_20210308T082953_036904_045743_2075',\n",
    "'S1A_IW_SLC__1SSH_20210320T082926_20210320T082953_037079_045D60_4C31'),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "autorift_jobs = sdk.Batch()\n",
    "for reference, secondary in autorift_pairs:\n",
    "    autorift_jobs += hyp3.submit_autorift_job(reference, secondary, name='autorift-example')\n",
    "print(autorift_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoRIFT does not currently accept any keyword arguments for product customization.\n",
    "\n",
    "## Monitoring jobs\n",
    "\n",
    "One jobs are submitted, you can either watch the jobs until they finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627372626e2c4e9caf947d4958507205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [timeout in 10800 s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rtc_jobs = hyp3.watch(rtc_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will require you to keep the cell/terminal running, or you can come back later and search for jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8431ac5cd5d744fbb4a7d0c315b88fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [timeout in 10800 s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rtc_jobs = hyp3.find_jobs(name='rtc-example')\n",
    "rtc_jobs = hyp3.watch(rtc_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading files\n",
    "\n",
    "Batches are collections of jobs. They provide a snapshot of the job status when the job was created or last\n",
    "refreshed. To get updated information on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 HyP3 Jobs: 0 succeeded, 0 failed, 0 running, 1 pending.\n",
      "1 HyP3 Jobs: 0 succeeded, 0 failed, 0 running, 1 pending.\n"
     ]
    }
   ],
   "source": [
    "print(autorift_jobs)\n",
    "autorift_jobs = hyp3.refresh(autorift_jobs)\n",
    "print(autorift_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54c2b12062f4267a53a54827724fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [timeout in 10800 s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autorift_jobs = hyp3.watch(autorift_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hyp3.watch()` will return a refreshed batch once every job in the batch has completed.\n",
    "\n",
    "Batches can be added together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'insar_jobs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Jobs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  RTC:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rtc_jobs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  InSAR:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43minsar_jobs\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  autoRIFT:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(autorift_jobs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m all_jobs \u001b[38;5;241m=\u001b[39m rtc_jobs \u001b[38;5;241m+\u001b[39m insar_jobs \u001b[38;5;241m+\u001b[39m autorift_jobs\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal number of Jobs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_jobs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'insar_jobs' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Number of Jobs:\\n  RTC:{len(rtc_jobs)}\\n  InSAR:{len(insar_jobs)}\\n  autoRIFT:{len(autorift_jobs)}')\n",
    "all_jobs = rtc_jobs + insar_jobs + autorift_jobs\n",
    "print(f'Total number of Jobs: {len(all_jobs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of a batch (at last refresh) by printing the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 HyP3 Jobs: 1 succeeded, 0 failed, 0 running, 0 pending.\n"
     ]
    }
   ],
   "source": [
    "print(autorift_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and filter jobs by status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of succeeded jobs: 1\n",
      "Number of failed jobs: 0\n"
     ]
    }
   ],
   "source": [
    "succeeded_jobs = autorift_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "print(f'Number of succeeded jobs: {len(autorift_jobs)}')\n",
    "failed_jobs = autorift_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "print(f'Number of failed jobs: {len(failed_jobs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the files for all successful jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1b59d265ad47aea5c4422dfbfdda39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f4b13fef69450bacc9aa797eacfa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1A_IW_SLC__1SSH_20210308T082926_20210308T082953_036904_045743_2075_X_S1A_IW_SLC__1SSH_20210320T082926_2021032…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_list = succeeded_jobs.download_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Note: only succeeded jobs will have files to download.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('S1A_IW_SLC__1SSH_20210308T082926_20210308T082953_036904_045743_2075_X_S1A_IW_SLC__1SSH_20210320T082926_20210320T082953_037079_045D60_4C31_G0120V02_P005_IL_ASF_OD.nc')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
